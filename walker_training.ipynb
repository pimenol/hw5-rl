{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff36817858940fd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T09:51:36.674524500Z",
     "start_time": "2023-11-30T09:51:36.663524700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn \n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c99faf649cd52b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Teaching a quadruped to walk\n",
    "\n",
    "Time to try out the learning algorithms that you just implemented on a more difficult problem. The WalkerEnv implements a quadruped robot kind-of thing, see for yourself. The goal is to move in the $x$ direction as fast and as far as possible.\n",
    "\n",
    "Your goal is to implement a class `WalkerPolicy` with function `determine_actions()` just like the StochasticPolicy we used earlier to control the pendulum. Below is a template of this class, but feel free to alter it however you want. The only important thing is the `determine_actions()` function!\n",
    "\n",
    "After you implement it, copy `WalkerPolicy` into a separate file `WalkerPolicy.py` that you will upload to BRUTE together with the (optional) learned weights in a zip file. How the policy is implemented is up to you! You are constrained to only the libraries we used so far though, such as torch, numpy etc..\n",
    "\n",
    "You will get some free points just for uploading a working policy (irrelevant of the performance). Further 2 points will be awarded for successfully traversing a small distance in the x direction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41290d3f9ccf033",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Hints\n",
    "\n",
    "There is no single easy way of doing this, but here are some suggestions on what you could try to improve your policy:\n",
    "\n",
    "1. This problem is much more difficult, than balancing a pendulum. It is a good idea to use a bit larger network than for the pendulum policy.\n",
    "\n",
    "2. You can also try to use a different optimizer, such as Adam and play with the hyperparameters.\n",
    "\n",
    "3. Using a neural network to compute the normal distribution scale $\\sigma$ can lead to too much randomness in the actions (i.e. exploration). You can use a fixed $\\sigma$ instead, or replace it with a learnable `torch.Parameter` initialized to some small constant. Make sure, you run it through an exponential, or softplus function to ensure $\\sigma$ is positive.\n",
    "\n",
    "4. The exploration can also be reduced by penalizing the variance of the action distribution in an additional loss term.\n",
    "\n",
    "5. If you see some undesirable behaviour, you can tweak the reward function to penalize it. Even though the $x$ distance is all we care about, adding extra terms to the reward can help guide the learning process (This is known as reward shaping). Simply define a reward function mapping the state $s_{t+1}$ and action $a_t$ to a scalar reward $r_t$ and put it in the config dictionary under the key `'reward_fcn'`. See the `WalkerEnv` class for the implementation of the default reward.\n",
    "\n",
    "6. Using the normal distribution on a bounded action space can lead to certain problems caused by action clipping. This can be mitigated by using a different distribution, such as the Beta distribution. See the `torch.distributions.beta` module for more information. (Note that Beta distribution is defined on the interval [0,1] and works better with parameters $\\alpha,\\beta \\geq 1$.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5f0cff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you cannot run with the visualization, you can set this to False\n",
    "VISUALIZE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d52d6512e1dc81e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T16:30:41.415964800Z",
     "start_time": "2023-11-30T16:30:40.816557700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from environment.WalkerEnv import WalkerEnv\n",
    "from WalkerPolicy import WalkerPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcd343db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_trajectories(env, policy, T=100):\n",
    "    states_np = env.vector_reset() \n",
    "    states = [states_np]\n",
    "    actions = []\n",
    "    rewards = []\n",
    "\n",
    "    for t in range(T):\n",
    "        s_torch = torch.tensor(states_np, dtype=torch.float32)\n",
    "        a_torch = policy.sample_actions(s_torch)       \n",
    "        a_np = a_torch.detach().cpu().numpy()\n",
    "\n",
    "        next_states_np, r_np = env.vector_step(a_np)  \n",
    "        actions.append(a_np)\n",
    "        rewards.append(r_np)\n",
    "        states.append(next_states_np)\n",
    "        states_np = next_states_np\n",
    "\n",
    "    states_t = torch.tensor(np.array(states), dtype=torch.float32)\n",
    "    actions_t = torch.tensor(np.array(actions), dtype=torch.float32) \n",
    "    rewards_t = torch.tensor(np.array(rewards), dtype=torch.float32)    \n",
    "    return states_t, actions_t, rewards_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "030dc17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(rewards, gamma=0.99):\n",
    "    T, N = rewards.shape\n",
    "    returns = torch.zeros_like(rewards)\n",
    "    running_sum = torch.zeros(N)\n",
    "    for t in reversed(range(T)):\n",
    "        running_sum = rewards[t] + gamma * running_sum\n",
    "        returns[t] = running_sum\n",
    "    return returns\n",
    "\n",
    "\n",
    "def compute_returns_and_advantages(\n",
    "    rewards, values, dones, gamma=0.99, lam=0.95\n",
    "):\n",
    "    T, N = rewards.shape\n",
    "    returns = np.zeros((T, N), dtype=np.float32)\n",
    "    advantages = np.zeros((T, N), dtype=np.float32)\n",
    "\n",
    "    gae = 0.0\n",
    "    for t in reversed(range(T)):\n",
    "        mask = 1.0 - dones[t]\n",
    "        delta = rewards[t] + gamma * values[t + 1] * mask - values[t]\n",
    "        gae = delta + gamma * lam * mask * gae\n",
    "        advantages[t] = gae\n",
    "        returns[t] = advantages[t] + values[t]\n",
    "\n",
    "    return returns, advantages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47349c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_walker(env, policy, epochs=100, T=1000, gamma=0.99, lr=1e-3):\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
    "\n",
    "    for e in range(epochs):\n",
    "        states, actions, rewards = sample_trajectories(env, policy, T=T)\n",
    "        T_, N_ = rewards.shape\n",
    "        \n",
    "        # values = policy.value_estimates(states)\n",
    "        G_t = compute_returns(rewards, gamma=gamma) \n",
    "        \n",
    "        flat_states = states[:T].reshape(T_ * N_, -1)\n",
    "        flat_actions = actions.reshape(T_ * N_, -1)\n",
    "        flat_returns = G_t.reshape(T_ * N_)\n",
    "\n",
    "        logp = policy.log_prob(flat_states, flat_actions)  \n",
    "        loss = -(logp * flat_returns).mean()\n",
    "        \n",
    "        # with torch.no_grad():\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_reward = rewards.mean().item()\n",
    "        print(f\"Epoch {e + 1}/{epochs}, Loss={loss.item():.3f}, AvgReward={avg_reward:.3f}\")\n",
    "\n",
    "    print(\"Training done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "421f1380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_walker(env, policy, T=1000):\n",
    "    obs = env.vector_reset() \n",
    "    N = env.num_walkers\n",
    "    total_reward = 0.0\n",
    "    max_x = obs[0, 0] if N == 1 else 0.0\n",
    "\n",
    "    for t in range(T):\n",
    "        a = policy.determine_actions(torch.tensor(obs).float())\n",
    "        obs, reward = env.vector_step(a.detach().numpy())\n",
    "        total_reward += reward.sum()       \n",
    "        if N == 1:\n",
    "            max_x = max(max_x, obs[0, 0])\n",
    "\n",
    "    print(f\"Test completed. Total reward = {total_reward:.2f}, Max X = {max_x:.2f}\")\n",
    "    return total_reward, max_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e4737a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment ready\n",
      "Epoch 1/50, Loss=-9.522, AvgReward=-0.033\n",
      "Epoch 2/50, Loss=-11.209, AvgReward=-0.039\n",
      "Epoch 3/50, Loss=-7.317, AvgReward=-0.022\n",
      "Epoch 4/50, Loss=-6.987, AvgReward=-0.023\n",
      "Epoch 5/50, Loss=-3.507, AvgReward=-0.012\n",
      "Epoch 6/50, Loss=2.550, AvgReward=0.009\n",
      "Epoch 7/50, Loss=2.487, AvgReward=0.006\n",
      "Epoch 8/50, Loss=8.020, AvgReward=0.024\n",
      "Epoch 9/50, Loss=3.209, AvgReward=0.008\n",
      "Epoch 10/50, Loss=7.481, AvgReward=0.024\n",
      "Epoch 11/50, Loss=10.780, AvgReward=0.034\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"N\": 16,            \n",
    "    \"vis\": False,      \n",
    "    \"track\": 0,        \n",
    "    # \"reward_fcn\": walker_reward\n",
    "}\n",
    "env = WalkerEnv(config)\n",
    "\n",
    "policy = WalkerPolicy(state_dim=29, action_dim=8)\n",
    "\n",
    "train_walker(env, policy, epochs=50, T=500, gamma=0.99, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b4a4844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-02 15:23:07.444 Python[75865:9347566] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-01-02 15:23:07.444 Python[75865:9347566] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n",
      "/Users/hella/.venv/lib/python3.12/site-packages/glfw/__init__.py:917: GLFWError: (65537) b'The GLFW library is not initialized'\n",
      "  warnings.warn(message, GLFWError)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test completed. Total reward = -0.62, Max X = 0.02\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"N\": 1,           \n",
    "    \"vis\": True,     \n",
    "    \"track\": 0,        \n",
    "    # \"reward_fcn\": walker_reward\n",
    "}\n",
    "env = WalkerEnv(config)\n",
    "test_walker(env, policy, T=500)\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
